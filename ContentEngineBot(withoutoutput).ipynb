{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNendrzOqtDd2qnBp8NP4Vf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/epicskills1/Alemeno-Assignment/blob/main/ContentEngineBot(withoutoutput).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract Text from pdfs"
      ],
      "metadata": {
        "id": "8Pi9XiMSkvhe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4oZaedZghgh_"
      },
      "outputs": [],
      "source": [
        "# Install PyMuPDF\n",
        "!pip install PyMuPDF\n",
        "\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text()\n",
        "    return text\n",
        "\n",
        "# Upload files\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Extract text from each PDF\n",
        "alphabet_text = extract_text_from_pdf(\"Alphabet_10K.pdf\")\n",
        "tesla_text = extract_text_from_pdf(\"Tesla_10K.pdf\")\n",
        "uber_text = extract_text_from_pdf(\"Uber_10K.pdf\")\n",
        "\n",
        "# Save extracted text to variables or files\n",
        "alphabet_text, tesla_text, uber_text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate Embeddings"
      ],
      "metadata": {
        "id": "t6zS3DuHkfgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install SentenceTransformers\n",
        "!pip install sentence-transformers\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "alphabet_embeddings = model.encode(alphabet_text.split('\\n'))\n",
        "tesla_embeddings = model.encode(tesla_text.split('\\n'))\n",
        "uber_embeddings = model.encode(uber_text.split('\\n'))\n",
        "\n",
        "# Save embeddings to variables\n",
        "alphabet_embeddings, tesla_embeddings, uber_embeddings\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4fFcuQf_ifiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Store in Vector Store using FAISS"
      ],
      "metadata": {
        "id": "VPybCul-nGY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install FAISS\n",
        "!pip install faiss-cpu\n",
        "\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "def create_faiss_index(embeddings):\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(np.array(embeddings))\n",
        "    return index\n",
        "\n",
        "alphabet_index = create_faiss_index(alphabet_embeddings)\n",
        "tesla_index = create_faiss_index(tesla_embeddings)\n",
        "uber_index = create_faiss_index(uber_embeddings)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-siy3z5HnEpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query the FAISS Index"
      ],
      "metadata": {
        "id": "Lx9wjlH_nzt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def query_index(index, query_embedding, texts):\n",
        "    D, I = index.search(query_embedding, k=5)\n",
        "    return [texts[i] for i in I[0]]\n",
        "\n",
        "def get_query_embedding(query):\n",
        "    return model.encode([query])\n",
        "\n",
        "# Example query\n",
        "query = \"What are the risk factors associated with Google and Tesla?\"\n",
        "query_embedding = get_query_embedding(query)\n",
        "\n",
        "alphabet_results = query_index(alphabet_index, query_embedding, alphabet_text.split('\\n'))\n",
        "tesla_results = query_index(tesla_index, query_embedding, tesla_text.split('\\n'))\n",
        "uber_results = query_index(uber_index, query_embedding, uber_text.split('\\n'))\n",
        "\n",
        "print(\"Alphabet Inc. Results:\")\n",
        "for result in alphabet_results:\n",
        "    print(result)\n",
        "\n",
        "print(\"\\nTesla, Inc. Results:\")\n",
        "for result in tesla_results:\n",
        "    print(result)\n",
        "\n",
        "print(\"\\nUber Technologies, Inc. Results:\")\n",
        "for result in uber_results:\n",
        "    print(result)\n"
      ],
      "metadata": {
        "id": "wiJEuEIWnxMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Integrate LLM"
      ],
      "metadata": {
        "id": "F_5e-j9Tof_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Transformers\n",
        "!pip install transformers\n",
        "\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "gpt_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "def generate_insights(prompt):\n",
        "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
        "    outputs = gpt_model.generate(inputs, max_length=150, num_return_sequences=1)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Example insights generation\n",
        "insights = generate_insights(query)\n",
        "print(\"\\nInsights:\")\n",
        "print(insights)\n"
      ],
      "metadata": {
        "id": "EZHN8HfJoesN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execution"
      ],
      "metadata": {
        "id": "a_tstcgGq6uw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pyngrok sentence-transformers faiss-cpu transformers PyMuPDF\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "YAvBLr7dq5ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " %%writefile app.py\n",
        "import streamlit as st\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import fitz  # PyMuPDF\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Function to extract text from PDFs\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text()\n",
        "    return text\n",
        "\n",
        "# Function to create a FAISS index\n",
        "def create_faiss_index(embeddings):\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(np.array(embeddings))\n",
        "    return index\n",
        "\n",
        "# Function to query the FAISS index\n",
        "def query_index(index, query_embedding, texts):\n",
        "    D, I = index.search(query_embedding, k=5)\n",
        "    return [texts[i] for i in I[0]]\n",
        "\n",
        "# Function to get query embeddings\n",
        "def get_query_embedding(query):\n",
        "    return model.encode([query])\n",
        "\n",
        "# Function to generate insights using GPT-2\n",
        "def generate_insights(prompt):\n",
        "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
        "    outputs = gpt_model.generate(inputs, max_length=150, num_return_sequences=1)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Load models\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "gpt_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "# Defining 'model' here, referencing the SentenceTransformer model\n",
        "model = embedding_model\n",
        "\n",
        "# Function to get query embeddings\n",
        "def get_query_embedding(query):\n",
        "    return model.encode([query])\n",
        "\n",
        "# Load and process PDF files\n",
        "alphabet_text = extract_text_from_pdf(\"Alphabet_10K (1).pdf\")\n",
        "tesla_text = extract_text_from_pdf(\"Tesla_10K (1).pdf\")\n",
        "uber_text = extract_text_from_pdf(\"Uber_10K (1).pdf\")\n",
        "\n",
        "alphabet_embeddings = embedding_model.encode(alphabet_text.split('\\n'))\n",
        "tesla_embeddings = embedding_model.encode(tesla_text.split('\\n'))\n",
        "uber_embeddings = embedding_model.encode(uber_text.split('\\n'))\n",
        "\n",
        "alphabet_index = create_faiss_index(alphabet_embeddings)\n",
        "tesla_index = create_faiss_index(tesla_embeddings)\n",
        "uber_index = create_faiss_index(uber_embeddings)\n",
        "\n",
        "# Streamlit UI\n",
        "st.title(\"Content Engine Chatbot\")\n",
        "\n",
        "query = st.text_input(\"Enter your query:\")\n",
        "if query:\n",
        "    query_embedding = get_query_embedding(query)\n",
        "\n",
        "    st.write(\"### Alphabet Inc.\")\n",
        "    alphabet_results = query_index(alphabet_index, query_embedding, alphabet_text.split('\\n'))\n",
        "    for result in alphabet_results:\n",
        "        st.write(result)\n",
        "\n",
        "    st.write(\"### Tesla, Inc.\")\n",
        "    tesla_results = query_index(tesla_index, query_embedding, tesla_text.split('\\n'))\n",
        "    for result in tesla_results:\n",
        "        st.write(result)\n",
        "\n",
        "    st.write(\"### Uber Technologies, Inc.\")\n",
        "    uber_results = query_index(uber_index, query_embedding, uber_text.split('\\n'))\n",
        "    for result in uber_results:\n",
        "        st.write(result)\n",
        "\n",
        "    st.write(\"### Insights\")\n",
        "    insights = generate_insights(query)\n",
        "    st.write(insights)\n"
      ],
      "metadata": {
        "id": "BDKQj-BarGr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pyngrok\n",
        "\n",
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token(\"2N13huDP2ANtbXSJQ7OQ2HvAShU_2pEtgBSQ4CM4B6mY6kTEF\") # Removed extra space before this line\n",
        "\n",
        "# Terminate all existing ngrok tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# Create a new ngrok tunnel, explicitly specifying HTTP protocol\n",
        "public_url = ngrok.connect(8501, proto=\"http\") # Specify protocol as \"http\"\n",
        "print(f\"Streamlit App URL: {public_url}\")"
      ],
      "metadata": {
        "id": "U8yuigpeufcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "subprocess.Popen(['streamlit', 'run', 'app.py'])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e-Z37ztDukBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RU_uuvmHx2Kb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
